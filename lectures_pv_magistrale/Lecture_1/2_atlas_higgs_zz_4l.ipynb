{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiSTHbA25ZtB"
   },
   "source": [
    "# Performing the Higgs -> ZZ -> 4 leptons search \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-aLTXX95ZtF"
   },
   "source": [
    "<a id='contents'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cest8B065ZtH"
   },
   "source": [
    "<a id='running'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dgRkVPu5ZtK"
   },
   "source": [
    "# Installation of packages not available by default on colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rjpPQFJHBHZO",
    "outputId": "4e4e4a10-448d-4576-a5a3-c9810c5fa386"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# update the pip package installer\n",
    "#%pip install --upgrade --user pip\n",
    "# install required packages\n",
    "#%pip install --upgrade --user uproot awkward vector numpy matplotlib\n",
    "\n",
    "!pip install uproot\n",
    "!pip install vector\n",
    "!pip install awkward\n",
    "!pip install hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrW2zmSv5ZtP"
   },
   "source": [
    "## Import packages used in the analysis\n",
    "\n",
    "We're going to be using a number of tools to help us:\n",
    "* uproot: lets us read .root files typically used in particle physics into data formats used in python\n",
    "* awkward: lets us store data as awkward arrays, a format that generalizes numpy to nested data with possibly variable length lists\n",
    "* vector: to allow vectorized 4-momentum calculations\n",
    "* numpy: provides numerical calculations such as histogramming\n",
    "* matplotlib: common tool for making plots, figures, images, visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tezkWvi5ZtP"
   },
   "outputs": [],
   "source": [
    "import uproot # for reading .root files\n",
    "import awkward as ak # to represent nested data in columnar format\n",
    "import vector # for 4-momentum calculations\n",
    "import time # to measure time to analyse\n",
    "import math # for mathematical functions such as square root\n",
    "import numpy as np # for numerical calculations such as histogramming\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1jsgyX25ZtR"
   },
   "source": [
    "## Data fraction, file path\n",
    "\n",
    "General definitions of fraction of data used, where to access the input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvTJwT3Fu0B8"
   },
   "outputs": [],
   "source": [
    "#lumi = 0.5 # fb-1 # data_A only\n",
    "#lumi = 1.9 # fb-1 # data_B only\n",
    "#lumi = 2.9 # fb-1 # data_C only\n",
    "#lumi = 4.7 # fb-1 # data_D only\n",
    "lumi = 10 # fb-1 # data_A,data_B,data_C,data_D\n",
    "fraction = 1 # reduce this is if you want the code to run quicker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQfajNQQ5ZtR"
   },
   "source": [
    "<a id='fraction'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7SvDjMB5ZtS"
   },
   "outputs": [],
   "source": [
    "fraction = 1. # reduce this is if you want the code to run quicker\n",
    "                                                                                                                                  \n",
    "tuple_path = \"https://atlas-opendata.web.cern.ch/atlas-opendata/samples/2020/4lep/\" # web address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygZz8Zeg5ZtU"
   },
   "source": [
    "Units, as stored in the data files.<br /> \n",
    "The quantities in the ATLAS ntuple are unfortunately stored in MeV.<br />\n",
    "Since the natural unit for LHC analysis is GeV, we define conversion factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlHtfEWO5ZtU"
   },
   "outputs": [],
   "source": [
    "MeV = 0.001\n",
    "GeV = 1.0\n",
    "mz = 91.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rwxT2a_u0CA"
   },
   "source": [
    "Define different classes of ntuples to access as a dictionary called \"samples\".<br />\n",
    "One has four types of ntuples:<br />\n",
    "<ul>\n",
    "<li>Data: the real data collected by ATLAS at the LHC\n",
    "<li>Reducible backgrounds: \n",
    "<li>Irreducible backgrounds\n",
    "<li>Signal\n",
    "</ul>\n",
    "The names in the list must correspond to the names of a sample defined in infofile.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpceF7lo5ZtT"
   },
   "outputs": [],
   "source": [
    "samples = {\n",
    "  'data': {\n",
    "    'list' : ['data_A','data_B','data_C','data_D'],\n",
    "  },\n",
    "  'reducible' : { # Z + ttbar\n",
    "     'list' : ['Zee','Zmumu','ttbar_lep'],\n",
    "     'color' : \"#6b59d3\" # purple\n",
    "  },\n",
    "  'irreducible' : { # ZZ\n",
    "     'list' : ['llll'],\n",
    "     'color' : \"#ff0000\" # red\n",
    "  },\n",
    "  'signal' : { # H -> ZZ -> llll\n",
    "     'list' : ['ggH125_ZZ4lep','VBFH125_ZZ4lep','WH125_ZZ4lep','ZH125_ZZ4lep'],\n",
    "     'color' : \"#00cdff\" # light blue\n",
    "  },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infofile.py contains for all the MC samples made available by ATLAS the factors necessary to calculate the normalisation of the MC samples to the data\n",
    "Print the first entry to show the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eXQ9WIxju0B4",
    "outputId": "57d3c8c1-0a61-4f59-90e8-98a4f9147281"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "if os.path.isfile('infofile.py')==False:\n",
    "  !wget https://raw.githubusercontent.com/atlas-outreach-data-tools/notebooks-collection-opendata/master/13-TeV-examples/uproot_python/infofile.py    \n",
    "import infofile    \n",
    "!head -n 12 infofile.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load into info vector contents of the file and for each sample calculate weight to apply to events when calculating statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elnlxPRBu0CB"
   },
   "outputs": [],
   "source": [
    "def get_xsec_weight(sample):\n",
    "  info = infofile.infos[sample] # open infofile\n",
    "  xsec_weight = (lumi*1000*info[\"xsec\"])/(info[\"sumw\"]*info[\"red_eff\"]) #*1000 to go from fb-1 to pb-1\n",
    "  return xsec_weight # return cross-section weight\n",
    "\n",
    "def calc_weight(xsec_weight, events):\n",
    "    return (\n",
    "        xsec_weight\n",
    "        * events.mcWeight\n",
    "        * events.scaleFactor_PILEUP\n",
    "        * events.scaleFactor_ELE\n",
    "        * events.scaleFactor_MUON\n",
    "        * events.scaleFactor_LepTRIGGER\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEWAYhJru0CB"
   },
   "outputs": [],
   "source": [
    "def get_data_from_files():\n",
    "\n",
    "  data = {} # define empty dictionary to hold dataframes\n",
    "  for s in samples: # loop over samples\n",
    "    print('Processing '+s+' samples') # print which sample\n",
    "    frames = [] # define empty list to hold data\n",
    "    for val in samples[s]['list']: # loop over each file\n",
    "      if s == 'data': prefix = \"Data/\" # Data prefix\n",
    "      else: # MC prefix\n",
    "        prefix = \"MC/mc_\"+str(infofile.infos[val][\"DSID\"])+\".\"\n",
    "      fileString = tuple_path+prefix+val+\".4lep.root\" # file name to open\n",
    "      temp = read_file(fileString,val) # call the function read_file defined below\n",
    "      frames.append(temp) # append dataframe returned from read_file to list of dataframes\n",
    "    data[s] = pd.concat(frames) # dictionary entry is concatenated dataframes\n",
    "\n",
    "  return data # return dictionary of dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MyurcAr5ZtV"
   },
   "source": [
    "Define function to calculate 4-lepton invariant mass.<br />\n",
    "It accesses the dataframe where the particle data are stored as scalar components, and creates an array of 4-vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oj8gJb4R5ZtV"
   },
   "outputs": [],
   "source": [
    "def calc_mllll(dff):\n",
    "#\n",
    "#   create array of 4-vectors, one for each lepton\n",
    "#\n",
    "    pl=[]\n",
    "    pl.append(vector.array({\"pt\":dff[\"ptl1\"], \"phi\":dff[\"phil1\"], \"eta\":dff[\"etal1\"], \"e\":dff[\"el1\"]}))\n",
    "    pl.append(vector.array({\"pt\":dff[\"ptl2\"], \"phi\":dff[\"phil2\"], \"eta\":dff[\"etal2\"], \"e\":dff[\"el2\"]}))\n",
    "    pl.append(vector.array({\"pt\":dff[\"ptl3\"], \"phi\":dff[\"phil3\"], \"eta\":dff[\"etal3\"], \"e\":dff[\"el3\"]}))\n",
    "    pl.append(vector.array({\"pt\":dff[\"ptl4\"], \"phi\":dff[\"phil4\"], \"eta\":dff[\"etal4\"], \"e\":dff[\"el4\"]}))\n",
    "    return (pl[0]+pl[1]+pl[2]+pl[3]).M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oj8gJb4R5ZtV"
   },
   "outputs": [],
   "source": [
    "def calc_ptl12(dff):\n",
    "#\n",
    "#   create array of 4-vectors, one for each lepton\n",
    "#\n",
    "    pl=[]\n",
    "    pl.append(vector.array({\"pt\":dff[\"ptl1\"], \"phi\":dff[\"phil1\"], \"eta\":dff[\"etal1\"], \"e\":dff[\"el1\"]}))\n",
    "    pl.append(vector.array({\"pt\":dff[\"ptl2\"], \"phi\":dff[\"phil2\"], \"eta\":dff[\"etal2\"], \"e\":dff[\"el2\"]}))\n",
    "    return (pl[0]+pl[1]).pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qip61stOu0CC"
   },
   "source": [
    "Simple template for a selection function acting on the dataframe, it creates a boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QR-1NL4f5ZtV"
   },
   "outputs": [],
   "source": [
    "def cut_ptl1(df):\n",
    "#\n",
    "    return (df.ptl1>5/MeV)\n",
    "def cut_ptl2(df):\n",
    "#\n",
    "    return (df.ptl2>20/MeV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-GwlBqK5ZtW"
   },
   "source": [
    "Define function to get data from files.\n",
    "\n",
    "The datasets used in this notebook have already been filtered to include at least 4 leptons per event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRmvqb0U5ZtW"
   },
   "outputs": [],
   "source": [
    "def read_file(path, sample):\n",
    "  with uproot.open( path + \":mini\") as tree:\n",
    "    numevents = tree.num_entries # number of events\n",
    "    if 'data' not in sample: xsec_weight = get_xsec_weight(sample) # get cross-section weight\n",
    "# variables to extract from ntuple\n",
    "    lepvar=[\"lep_pt\", \"lep_eta\", \"lep_phi\",\"lep_E\",\n",
    "          \"lep_charge\",\"lep_type\",\n",
    "          \"lep_ptcone30\",\"lep_etcone20\", \"lep_isTightID\"]\n",
    "    lepwei=[MeV,1.,1.,MeV,\n",
    "            1.,1.]\n",
    "    weivar=[\"mcWeight\",\"scaleFactor_PILEUP\",\n",
    "            \"scaleFactor_ELE\",\"scaleFactor_MUON\",\n",
    "            \"scaleFactor_LepTRIGGER\"] # variables to calculate Monte Carlo weight\n",
    "\n",
    "    tupvar=weivar+lepvar\n",
    "# read tree into awkward array with uproot\n",
    "    lep_momentum = tree.arrays(tupvar,entry_stop=numevents*fraction,library=\"ak\")\n",
    "# define the names of the variables to go into dataframe\n",
    "    colnam=[\"ptl1\",\"etal1\",\"phil1\",\"el1\",\"chl1\",\"typl1\",\"ptcl1\",\"etcl1\",\"tightl1\",\n",
    "          \"ptl2\",\"etal2\",\"phil2\",\"el2\",\"chl2\",\"typl2\",\"ptcl2\",\"etcl2\",\"tightl2\",\n",
    "          \"ptl3\",\"etal3\",\"phil3\",\"el3\",\"chl3\",\"typl3\",\"ptcl3\",\"etcl3\",\"tightl3\",\n",
    "          \"ptl4\",\"etal4\",\"phil4\",\"el4\",\"chl4\",\"typl4\",\"ptcl4\",\"etcl4\",\"tightl4\"]\n",
    "# create numpy vector assuming that lep_ arrays have 4 components\n",
    "    for i in range(0,4):\n",
    "      for j in range(0,len(lepvar)):\n",
    "        if i==0 and j==0:\n",
    "          ptlep=ak.to_numpy(lep_momentum[lepvar[j]][:,i])\n",
    "        else:\n",
    "          ptlep=np.vstack([ptlep,ak.to_numpy(lep_momentum[lepvar[j]][:,i])])\n",
    "    print(ptlep.shape)\n",
    "# end up with numpy 2d vector with n_var rows and n_event columns\n",
    "# to transpose, as in pandas 'features' are columns and 'observations' rows\n",
    "    ptlep=ptlep.transpose()\n",
    "# create dataframe \n",
    "    df = pd.DataFrame(ptlep)\n",
    "# add names of columns\n",
    "    df.columns=colnam    \n",
    "    if 'data' not in sample: # only do this for Monte Carlo simulation files\n",
    "# multiply all Monte Carlo weights and scale factors together to give total weight\n",
    "      df[\"totalWeight\"] = calc_weight(xsec_weight, lep_momentum)    \n",
    "# add new derived variable\n",
    "    df[\"mllll\"]=calc_mllll(df)\n",
    "# apply cuts on variable\n",
    "    df=df[cut_ptl1(df)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwFgiBZ35ZtY"
   },
   "source": [
    "This is where the processing happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZerdO-d5ZtY",
    "outputId": "b60b1980-25a5-424f-9ba2-bbfd4ab169ab",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time.time() # time at start of whole processing\n",
    "df = get_data_from_files() #process all files\n",
    "print(df.keys())\n",
    "elapsed = time.time() - start # time after whole processing\n",
    "print(\"Time taken: \"+str(round(elapsed,1))+\"s\") # print total time taken to process every file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWOGZ50Uu0CF"
   },
   "source": [
    "Define routine which loops over all the samples in the dictionary of dataframes and applies\n",
    "selections on them. <br />\n",
    "It would be simpler to aplly the cuts when reading the files, but in this way you can play with \n",
    "the selections without the need of reloading the files every time<br />\n",
    "One can also create dataframes with different selection levels and compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eFKlNfmX5Zta",
    "outputId": "ced0ec82-c008-4dbf-8de6-ec9e1dcc1cf7"
   },
   "outputs": [],
   "source": [
    "def apply_selections(df):\n",
    "\n",
    "  data = {} # define empty dictionary to hold dataframes\n",
    "  for s in samples: # loop over samples\n",
    "    print('Apply selections on '+s+' samples') # print which sample\n",
    "    print('before', df[s].shape) \n",
    "    seldf=df[s]\n",
    "#    seldf=seldf[cut_ptl1(df[s])]\n",
    "    seldf=seldf.query('ptl1>5000')\n",
    "    print('after',seldf.shape)\n",
    "    data[s]=seldf\n",
    "#    data[s] = pd.concat(frames) # dictionary entry is concatenated dataframes\n",
    "\n",
    "  return data # return dictionary of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_columns(df):\n",
    "\n",
    "  data = {} # define empty dictionary to hold dataframes\n",
    "  for s in samples: # loop over samples\n",
    "    print('Add columns '+s+' samples') # print which sample\n",
    "    print('before', df[s].shape) \n",
    "    add_df=df[s] \n",
    "    add_df[\"ptll12\"]=calc_ptl12(add_df)\n",
    "    data[s]=add_df\n",
    "\n",
    "  return data # return dictionary of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate events for available samples in dataframe dictionary normalised\n",
    "# to available statistics\n",
    "def calc_events(df):\n",
    "  stat = {} # define empty dictionary to hold stat\n",
    "  for s in samples: # loop over samples\n",
    "    if s not in ['data']: \n",
    "      print('Calculate events on '+s+' samples') # print which sample\n",
    "      stat[s]=(df[s]['totalWeight']).sum()\n",
    "    if s in ['data']:\n",
    "      stat[s]=df[s].shape[0]\n",
    "  return stat # return dictionary of statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finstat=calc_events(df)\n",
    "nsig=finstat['signal']\n",
    "nbg=finstat['reducible']+finstat['irreducible']\n",
    "print(nsig,nbg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply selection and store it in differently named dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eFKlNfmX5Zta",
    "outputId": "ced0ec82-c008-4dbf-8de6-ec9e1dcc1cf7"
   },
   "outputs": [],
   "source": [
    "dfs=apply_selections(df)\n",
    "df=add_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finstat1=calc_events(dfs)\n",
    "nsig1=finstat1['signal']\n",
    "nbg1=finstat1['reducible']+finstat1['irreducible']\n",
    "print(nsig1,nbg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt, log\n",
    "def calc_sig(nsig, nbg, nbExpEr):\n",
    "    nbObs=nsig+nbg\n",
    "    nbExp=nbg\n",
    "    if nbExp>0 and nbExpEr>0:\n",
    "      factor1 = nbObs*log( (nbObs*(nbExp+nbExpEr**2))/(nbExp**2+nbObs*nbExpEr**2) )\n",
    "      factor2 = (nbExp**2/nbExpEr**2)*log( 1 + (nbExpEr**2*(nbObs-nbExp))/(nbExp*(nbExp+nbExpEr**2)) )\n",
    "      signi  = sqrt(2*(factor1 - factor2))\n",
    "    if nbExp>0 and nbExpEr==0:\n",
    "      signi=sqrt(2*((nsig+nbg)*log(1+nsig/nbg)-nsig))\n",
    "    if nbExp==0 and nbExpEr==0:\n",
    "      signi=sqrt(nsig)\n",
    "    if nbExp==0 and nbExpEr>0:\n",
    "      signi=nsig/sqrt(nbExpEr)\n",
    "    return signi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"significance is \",calc_sig(nsig1,nbg1,0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzk5n_pc5Zta"
   },
   "source": [
    "## Plotting\n",
    "\n",
    "Set simple wrapper for plotting histograms with matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5vLBRJXu0CG"
   },
   "outputs": [],
   "source": [
    "# in input:  df=dataframe var=variable in dataframe, \n",
    "# scale=scale factor on value of variable (e.g. for scaling between MeV and GeV)\n",
    "# nbin,xlow,xhigh: limits of histogram\n",
    "# norm: choose whether you plot acutal number of events in bin or fraction of events \n",
    "#       in each bin with respect to total\n",
    "# labx, laby labels of x and y axis of histogram \n",
    "# title: title of hisotgram\n",
    "# leg: legenda for content, useful if you superimpose two hists\n",
    "def hist1d(df, var, scale, nbin, xlow, xhig, norm, labx, laby, title, leg):\n",
    "  bins = np.linspace(xlow,xhig,nbin)\n",
    "  plt.hist(df[var]*scale, bins, alpha=0.5, density=norm, label=leg)\n",
    "  plt.xlabel(labx)\n",
    "  plt.ylabel(laby)\n",
    "  plt.legend(loc='best')\n",
    "# 2d, same definition of inputs as above\n",
    "def hist2d(df,var1,var2,scalex,scaley,nbinx,xlow,xhig,nbiny,ylow,yhig,labx,laby,title):\n",
    "  binx = np.linspace(xlow,xhig,nbinx)\n",
    "  biny = np.linspace(ylow,yhig,nbiny)\n",
    "  plt.hist2d(df[var1]*scalex, df[var2]*scaley, bins=[binx, biny], density=False, cmin=0.5)\n",
    "  plt.xlabel(labx)\n",
    "  plt.ylabel(laby)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of plotting a couple of basic variables for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AQ3untsTu0CG",
    "outputId": "03daf22c-4131-4575-c55a-ad0e02546e95"
   },
   "outputs": [],
   "source": [
    "hist2d(df['reducible'],\"ptl1\",\"ptl2\",MeV,MeV,100,0.,200.,100,0.,200.,'ptl1 [GeV]','ptl2 [GeV]','')\n",
    "plt.show()\n",
    "hist1d(df['irreducible'],\"ptl3\",MeV,100,0.,200.,False,'ptl3 [GeV]','Number of events','','no cuts lep')\n",
    "plt.show()\n",
    "hist1d(dfs['reducible'],\"ptl4\",MeV,100,0.,200.,False,'ptl4 [GeV]','Number of events','','cuts on lep')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing ATLAS  publication plots\n",
    "\n",
    "Plot the distribution of the invariant mass of 4 leptons for backgrounds, data, and signals based on the graphical style of ATLAS<br />\n",
    "Backgrounds and signal are stacked, to provide direct comparison with data.<br /> \n",
    "Slightly adapted from the original ATLAS notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qEP2cEb-u0CG",
    "outputId": "27c39032-a6ce-4740-8e79-0845e381972c"
   },
   "outputs": [],
   "source": [
    "from matplotlib.ticker import AutoMinorLocator # for minor ticks\n",
    "def plot_data(data, var, scale, xmin, xmax, step_size, labx):\n",
    "  \n",
    "#  xmin = 0.*GeV\n",
    "#  xmax = 200.*GeV\n",
    "#  step_size = 5.*GeV\n",
    "#  scale=0.001\n",
    "  bin_edges = np.arange(start=xmin, # The interval includes this value\n",
    "                   stop=xmax+step_size, # The interval doesn't include this value\n",
    "                   step=step_size ) # Spacing between values\n",
    "  bin_centres = np.arange(start=xmin+step_size/2, # The interval includes this value\n",
    "                          stop=xmax+step_size/2, # The interval doesn't include this value\n",
    "                          step=step_size ) # Spacing between values\n",
    "\n",
    "  data_x,_ = np.histogram((data['data'][var])*scale,\n",
    "                          bins=bin_edges ) # histogram the data\n",
    "  data_x_errors = np.sqrt( data_x ) # statistical error on the data\n",
    "\n",
    "  signal_x = (data['signal'][var])*scale # histogram the signal\n",
    "  signal_weights = data['signal'][\"totalWeight\"] # get the weights of the signal events\n",
    "  signal_color = samples['signal']['color'] # get the colour for the signal bar\n",
    "\n",
    "  mc_x = [] # define list to hold the Monte Carlo histogram entries\n",
    "  mc_weights = [] # define list to hold the Monte Carlo weights\n",
    "  mc_colors = [] # define list to hold the colors of the Monte Carlo bars\n",
    "  mc_labels = [] # define list to hold the legend labels of the Monte Carlo bars\n",
    "\n",
    "  for s in samples: # loop over samples\n",
    "      if s not in ['data', 'signal']: # if not data nor signal\n",
    "          mc_x.append((data[s][var])*scale) # append to the list of Monte Carlo histogram entries\n",
    "          mc_weights.append(data[s][\"totalWeight\"]) # append to the list of Monte Carlo weig \n",
    "          mc_colors.append( samples[s]['color'] ) # append to the list of Monte Carlo bar colors\n",
    "          mc_labels.append( s ) # append to the list of Monte Carlo legend labels\n",
    "\n",
    "  # *************\n",
    "  # Main plot\n",
    "  # *************\n",
    "  main_axes = plt.gca() # get current axes\n",
    "\n",
    "  # plot the data points\n",
    "  main_axes.errorbar(x=bin_centres, y=data_x, yerr=data_x_errors,\n",
    "                     fmt='ko', # 'k' means black and 'o' is for circles\n",
    "                     label='Data')\n",
    "\n",
    "  # plot the Monte Carlo bars\n",
    "  mc_heights = main_axes.hist(mc_x, bins=bin_edges,\n",
    "                              weights=mc_weights, stacked=True,\n",
    "                              color=mc_colors, label=mc_labels )\n",
    "\n",
    "  \n",
    "  mc_x_tot = mc_heights[0][-1] # stacked background MC y-axis value\n",
    "  # calculate MC statistical uncertainty: sqrt(sum w^2)\n",
    "  mc_x_err = np.sqrt(np.histogram(np.hstack(mc_x), bins=bin_edges, weights=np.hstack(mc_weights)**2)[0])\n",
    "\n",
    "  # plot the signal bar\n",
    "  main_axes.hist(signal_x, bins=bin_edges, bottom=mc_x_tot,\n",
    "                 weights=signal_weights, color=signal_color,\n",
    "                 label=r'Signal ($m_H$ = 125 GeV)')\n",
    "\n",
    "  # plot the statistical uncertainty\n",
    "  main_axes.bar(bin_centres, # x\n",
    "                2*mc_x_err, # heights\n",
    "                alpha=0.5, # half transparency\n",
    "                bottom=mc_x_tot-mc_x_err, color='none',\n",
    "                hatch=\"////\", width=step_size, label='Stat. Unc.' )\n",
    "\n",
    "  # set the x-limit of the main axes\n",
    "  main_axes.set_xlim( left=xmin, right=xmax )\n",
    "\n",
    "  # separation of x axis minor ticks\n",
    "  main_axes.xaxis.set_minor_locator( AutoMinorLocator() )\n",
    "\n",
    "  # set the axis tick parameters for the main axes\n",
    "  main_axes.tick_params(which='both', # ticks on both x and y axes\n",
    "                        direction='in', # Put ticks inside and outside the axes\n",
    "                        top=True, # draw ticks on the top axis\n",
    "                        right=True ) # draw ticks on right axis\n",
    "\n",
    "  # x-axis label\n",
    "  main_axes.set_xlabel(labx,\n",
    "                      fontsize=13, x=1, horizontalalignment='right' )\n",
    "\n",
    "  # write y-axis label for main axes\n",
    "  main_axes.set_ylabel('Events / '+str(step_size)+' GeV',\n",
    "                       y=1, horizontalalignment='right')\n",
    "\n",
    "  # set y-axis limits for main axes\n",
    "  main_axes.set_ylim( bottom=0, top=np.amax(data_x)*1.6 )\n",
    "\n",
    "  # add minor ticks on y-axis for main axes\n",
    "  main_axes.yaxis.set_minor_locator( AutoMinorLocator() )\n",
    "\n",
    "  # Add text 'ATLAS Open Data' on plot\n",
    "\n",
    "  plt.text(0.05, # x\n",
    "           0.93, # y\n",
    "           'ATLAS Open Data', # text\n",
    "           transform=main_axes.transAxes, # coordinate system used is that of main_axes\n",
    "           fontsize=13 )\n",
    "\n",
    "  # Add text 'for education' on plot\n",
    "  plt.text(0.05, # x\n",
    "           0.88, # y\n",
    "           'for education', # text\n",
    "           transform=main_axes.transAxes, # coordinate system used is that of main_axes\n",
    "           style='italic',\n",
    "           fontsize=8 )\n",
    "\n",
    "  # Add energy and luminosity\n",
    "  lumi_used = str(lumi*fraction) # luminosity to write on the plot\n",
    "  plt.text(0.05, # x\n",
    "           0.82, # y\n",
    "           '$\\sqrt{s}$=13 TeV,$\\int$L dt = '+lumi_used+' fb$^{-1}$', # text\n",
    "           transform=main_axes.transAxes ) # coordinate system used is that of main_axes\n",
    "\n",
    "  # Add a label for the analysis carried out\n",
    "  plt.text(0.05, # x\n",
    "           0.76, # y           \n",
    "           r'$H \\rightarrow ZZ^* \\rightarrow 4\\ell$', # text\n",
    "           transform=main_axes.transAxes ) # coordinate system used is that of main_axes\n",
    "\n",
    "  # draw the legend\n",
    "  main_axes.legend( frameon=False ) # no box around the legend\n",
    "  plt.show()\n",
    "  return\n",
    "plot_data(df,'mllll',MeV,80.,250.,5., r'm_{llll} [GeV]')\n",
    "plot_data(dfs,'mllll',MeV,80.,250.,5., r'm_{llll} [GeV]')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpMrzx4E5Ztd"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "* Add more cuts from the [Higgs discovery paper](https://www.sciencedirect.com/science/article/pii/S037026931200857X#se0040) trying to get to a version of Figure 2 of the paper similar to the published one (hint: concentrate on the lepton id cuts)\n",
    "* Calculate the efficiency of the cuts on the various data classes\n",
    "* Get the estimated numbers of events, like [Table 3 of the Higgs discovery paper](https://www.sciencedirect.com/science/article/pii/S037026931200857X#tl0030)\n",
    "* Calculate the mass of the leading and subleading lepton pair according to the definition of the paper\n",
    "   * Looping on events (relatively easy)\n",
    "   * Using the vector algebra of numpy\n",
    "* Add a plot to show the invariant mass distribution of the sub-leading lepton pair, like [Figure 1 of the Higgs discovery paper](https://www.sciencedirect.com/science/article/pii/S037026931200857X#fg0010)\n",
    "* Add a plot of m12 against m34, like [Figure 3 of the Higgs discovery paper](https://www.sciencedirect.com/science/article/pii/S037026931200857X#fg0030)\n",
    "* Your idea!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
